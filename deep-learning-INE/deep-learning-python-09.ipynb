{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Activation Functions\n",
    "\n",
    "In this notebook, we'll use the MNIST dataset to explore activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MNIST data functions\n",
    "# matplotlib for plotting\n",
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the MNIST Dataset again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mnist.load_data() will automatically download the dataset if you don't have it\n",
    "(MNIST_train_X, MNIST_train_y), (MNIST_test_X, MNIST_test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of preprocessing on the data before we can train (teach) our network\n",
    "\n",
    "For today, just ignore this.  Consider it a \"Necessary evil\". It's really not evil, but it is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_X = MNIST_train_X.reshape((60000, 28 * 28))\n",
    "MNIST_train_X = MNIST_train_X.astype('float32') / 255\n",
    "\n",
    "MNIST_test_X = MNIST_test_X.reshape((10000, 28 * 28))\n",
    "MNIST_test_X = MNIST_test_X.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MNIST_train_y = to_categorical(MNIST_train_y)\n",
    "MNIST_test_y = to_categorical(MNIST_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same old topology, minus some activation functions\n",
    "\n",
    "So, now we can mess around with the activation functions.  We know that `relu` introduces nonlinearities. \n",
    "Let's do a few simple experiments\n",
    "\n",
    "* let's try *removing* the relu function. \n",
    "* remove softmax function on the final layer\n",
    "* try sigmoid instead of relu (with softmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "# we can adjust the activation function below.\n",
    "network.add(layers.Dense(512, activation='sigmoid' , input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation=None))\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',# change this parameter here.\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record your results here\n",
    "\n",
    "None + Softmax : \n",
    "\n",
    "None + None : \n",
    "\n",
    "Sig + Softmax : \n",
    "\n",
    "Sig + None  : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does None + Softmax do so much better than None + None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression - no final layer activation.\n",
    "\n",
    "In the next section of the course we will begin working on a regression problem.  Regression is when we want to predict a continous value, like the price of a house.  This case we would not want to use a final layer activation, because it would \"squish\" everything between 0,1 (like sigmoid or softmax).  So keep this in mind, when your objective changes, you need to change your topology to match it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Experiment\n",
    "\n",
    "Keras has a number of different activation functions.  Go ahead and try some others:\n",
    "\n",
    "https://keras.io/activations/\n",
    "\n",
    "And try mixing it up with multiple layers.  Can you find a way to get a higher accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "#input layer\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "\n",
    "# let's add another layer -- keep the activation function as 'relu'\n",
    "network.add(layers.Dense(512, activation=''))\n",
    "network.add(layers.Dense(284, activation=''))\n",
    "network.add(layers.Dense(128, activation=''))\n",
    "\n",
    "#output layer\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record your results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

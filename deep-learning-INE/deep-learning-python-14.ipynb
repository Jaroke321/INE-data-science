{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data\n",
    "\n",
    "In this notebook we'll use the IMBD dataset to practice making validation data.\n",
    "Then we'll tune our hyper-parameters on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "C:\\Users\\Jacob\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Jacob\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset and load it into the familiar sets of data.  Note the keyword argument `num_words = 10000` this built-in argument means that we are only taking the 10,000 most commonly used words in the dataset.  The logic is that rarely used words aren't going to help classify the movies as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (25000,)\n",
      "Test shape :  (25000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Train shape : \",train_data.shape)\n",
    "print (\"Test shape : \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the validation data\n",
    "\n",
    "I don't like to write my own splitting functions, because I'm always afraid I'll make a dumb indexing mistake.  For that reason I highly suggest using scikit-learns inbuilt functions.  \n",
    "\n",
    "We will use train test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training data is a 1-tensor, that means each sample is just a list of numbers.  Let's look a a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid  = train_test_split(train_data, train_labels, test_size = .35, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on random states\n",
    "\n",
    "The `random_state` parameter of many algorithms (classifiers / models) in sklearn are used to \"freeze\" your model, so it can be reproduced over and over.  Models with a `random_state` set are **Not** random.  Sometimes models or methods, require an element of randomness.  For example shuffling and splitting the data is required to be random.  However, since the results change ever so slightly each time you re-run the method -- it can be frustrating (scores will change slightly).  For this reason, I'm setting the random state here, so you can reproduce it on your computer.\n",
    "\n",
    "By assigning a `random_state = 1` variable in the argument to the method -- we _actually remove all randomness_ from the method.  In essence, the algorithm becomes deterministic (fixed) and the results will always be the same.\n",
    "\n",
    "I don't like this in general.\n",
    "Why?\n",
    "\n",
    "Because it creates a slice of the model, which isn't representative of the real model. The real model has randomized parameters, so it should randomly change every time you instantiate it.  This means that typically to get real results, you need to run the model multiple time (hundreds) in order to know _in general_ how it will perform.\n",
    "\n",
    "When the difference between two models is <1%, can we be sure one model is better than the other?  Especially when we've randomly created a static version of that model by freezing it's random variables?  It's not really possible.  So I would advise that the better solution would be to run the experiment (with randomness) a few hundred times and then average the results.  This would be a much better indicator of what kind of performance you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape : (16250,)\n",
      "y Train shape : (16250,)\n",
      "X Valid shape : (8750,)\n",
      "y valid shape : (8750,)\n"
     ]
    }
   ],
   "source": [
    "print (\"X Train shape : {}\".format(X_train.shape))\n",
    "print (\"y Train shape : {}\".format(y_train.shape))\n",
    "print (\"X Valid shape : {}\".format(X_valid.shape))\n",
    "print (\"y valid shape : {}\".format(y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have our validation data, let's do some experiments.\n",
    "\n",
    "Last time we were trying to figure out which one was better\n",
    "\n",
    "* bag of words with binary features\n",
    "* bag of words with count features\n",
    "* bag of worts with TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_bag_of_words (sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bag_of_words (sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] += 1.\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make 3 different datasets to try out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_X_train = binary_bag_of_words(X_train)\n",
    "binary_X_valid = binary_bag_of_words(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_X_train = count_bag_of_words(X_train)\n",
    "count_X_valid = count_bag_of_words(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_X_train = transformer.fit_transform(count_X_train)\n",
    "tfidf_X_valid = transformer.transform(count_X_valid)\n",
    "\n",
    "tfidf_X_train = tfidf_X_train.toarray()\n",
    "tfidf_X_valid= tfidf_X_valid.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One helpful *magic* function -- `whos`\n",
    "\n",
    "we can search for variables with `whos` and see what we've created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable         Type       Data/Info\n",
      "-------------------------------------\n",
      "X_train          ndarray    16250: 16250 elems, type `object`, 130000 bytes (126.953125 kb)\n",
      "X_valid          ndarray    8750: 8750 elems, type `object`, 70000 bytes\n",
      "binary_X_train   ndarray    16250x10000: 162500000 elems, type `float64`, 1300000000 bytes (1239.776611328125 Mb)\n",
      "binary_X_valid   ndarray    8750x10000: 87500000 elems, type `float64`, 700000000 bytes (667.572021484375 Mb)\n",
      "count_X_train    ndarray    16250x10000: 162500000 elems, type `float64`, 1300000000 bytes (1239.776611328125 Mb)\n",
      "count_X_valid    ndarray    8750x10000: 87500000 elems, type `float64`, 700000000 bytes (667.572021484375 Mb)\n",
      "test_data        ndarray    25000: 25000 elems, type `object`, 200000 bytes (195.3125 kb)\n",
      "test_labels      ndarray    25000: 25000 elems, type `int64`, 200000 bytes (195.3125 kb)\n",
      "tfidf_X_train    ndarray    16250x10000: 162500000 elems, type `float64`, 1300000000 bytes (1239.776611328125 Mb)\n",
      "tfidf_X_valid    ndarray    8750x10000: 87500000 elems, type `float64`, 700000000 bytes (667.572021484375 Mb)\n",
      "train_data       ndarray    25000: 25000 elems, type `object`, 200000 bytes (195.3125 kb)\n",
      "train_labels     ndarray    25000: 25000 elems, type `int64`, 200000 bytes (195.3125 kb)\n",
      "y_train          ndarray    16250: 16250 elems, type `int64`, 130000 bytes (126.953125 kb)\n",
      "y_valid          ndarray    8750: 8750 elems, type `int64`, 70000 bytes\n"
     ]
    }
   ],
   "source": [
    "whos ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_run_model(X_train, X_valid, y_train = y_train, y_valid = y_valid):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs = 15, batch_size = 512)\n",
    "    valid_loss, valid_acc = model.evaluate(X_valid, y_valid)\n",
    "    print('valid_acc:', valid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run all three models with the different data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5182 - accuracy: 0.7821\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3127 - accuracy: 0.9035\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2316 - accuracy: 0.9263\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1872 - accuracy: 0.9385\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1527 - accuracy: 0.9529\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1271 - accuracy: 0.9634\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1085 - accuracy: 0.9672\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0915 - accuracy: 0.9728\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9794\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0641 - accuracy: 0.9844\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0544 - accuracy: 0.9856\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0448 - accuracy: 0.9890\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.9913\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0280 - accuracy: 0.9950\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9944\n",
      "274/274 [==============================] - 0s 766us/step - loss: 0.5044 - accuracy: 0.8640\n",
      "valid_acc: 0.8640000224113464\n"
     ]
    }
   ],
   "source": [
    "build_run_model(binary_X_train, binary_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5505 - accuracy: 0.7506\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3503 - accuracy: 0.8850\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2673 - accuracy: 0.9124\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2145 - accuracy: 0.9306\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1835 - accuracy: 0.9413\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1598 - accuracy: 0.9478\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1346 - accuracy: 0.9578\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1151 - accuracy: 0.9655\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0985 - accuracy: 0.9711\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0862 - accuracy: 0.9762\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0727 - accuracy: 0.9785\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0620 - accuracy: 0.9828\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0541 - accuracy: 0.9868\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0467 - accuracy: 0.9886\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9908\n",
      "274/274 [==============================] - 0s 840us/step - loss: 0.4371 - accuracy: 0.8755\n",
      "valid_acc: 0.8755428791046143\n"
     ]
    }
   ],
   "source": [
    "build_run_model(count_X_train, count_X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.6433 - accuracy: 0.7636\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.8753\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4064 - accuracy: 0.8993\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.3169 - accuracy: 0.9137\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2538 - accuracy: 0.9246\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.2099 - accuracy: 0.9346\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1773 - accuracy: 0.9430\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1522 - accuracy: 0.9505\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1329 - accuracy: 0.9568\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1161 - accuracy: 0.9645\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 0.9697\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9735\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0782 - accuracy: 0.9777\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0687 - accuracy: 0.9816\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9853\n",
      "274/274 [==============================] - 0s 779us/step - loss: 0.3215 - accuracy: 0.8838\n",
      "valid_acc: 0.8837714195251465\n"
     ]
    }
   ],
   "source": [
    "build_run_model(tfidf_X_train, tfidf_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which one is best?\n",
    "\n",
    "It doesn't actually matter, what's important is that you get the idea of tuning your parameters on the **validation** data so we don't accidentally overfit to the testing data.\n",
    "\n",
    "Now of course, we could tune it a lot, and get the best results on the validation data, and this might result in bad test scores!  Why?  Because then we've overfit to the **validation** data, which is pretty bad too!\n",
    "\n",
    "So, in the next lesson we'll start looking at how to detect overfitting so we can stop ourselves when we are doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

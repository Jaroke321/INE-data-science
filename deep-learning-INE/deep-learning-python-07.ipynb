{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Optimizers\n",
    "\n",
    "In this notebook, we'll use the MNIST dataset to explore different optimizers and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MNIST data functions\n",
    "# matplotlib for plotting\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the MNIST Dataset again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mnist.load_data() will automatically download the dataset if you don't have it\n",
    "(MNIST_train_X, MNIST_train_y), (MNIST_test_X, MNIST_test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of preprocessing on the data before we can train (teach) our network\n",
    "\n",
    "For today, just ignore this.  Consider it a \"Necessary evil\". It's really not evil, but it is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_X = MNIST_train_X.reshape((60000, 28 * 28))\n",
    "MNIST_train_X = MNIST_train_X.astype('float32') / 255\n",
    "\n",
    "MNIST_test_X = MNIST_test_X.reshape((10000, 28 * 28))\n",
    "MNIST_test_X = MNIST_test_X.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MNIST_train_y = to_categorical(MNIST_train_y)\n",
    "MNIST_test_y = to_categorical(MNIST_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the architecture of the network\n",
    "\n",
    "Let's stick with the same one from the last excercise, this way we can see how it changes with different loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Optimizers with default settings\n",
    "Now we'd like to compare some optimizers\n",
    "Let's take a look at whats available in the box from Keras\n",
    "\n",
    "https://keras.io/optimizers/\n",
    "\n",
    "There are a bunch of optimizers here!\n",
    "\n",
    "Just like loss functions, there are a lot of choices and options.  Unless you spend time reading the papers on how each one was made, you pretty much should just stick with default options.  But, we can always experiment and try some things out.\n",
    "\n",
    "I suggest you try\n",
    "\n",
    "* `SGD` - One of the earlier optimizers, no paper to support it.\n",
    "* `RMSProp` - recommended in \"deep learning python\" book by manning.\n",
    "* `adam` \n",
    "\n",
    "\n",
    "For all these optimizers, we'll follow the keras instructions and leave everything except for the learning rate at default.\n",
    "\n",
    "In fact for now let's leave it all default just to compare the 3 optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.2610 - accuracy: 0.9243\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1040 - accuracy: 0.9689\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0684 - accuracy: 0.9796\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0493 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0377 - accuracy: 0.9890\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0744 - accuracy: 0.9775\n",
      "test_acc: 0.9775000214576721\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "network.compile(optimizer='RMSProp', #your value here\n",
    "                loss='categorical_crossentropy',# change this parameter here.\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rmsprop : 0.9775\n",
    "\n",
    "SGD : 0.9135\n",
    "\n",
    "Adam : 0.9750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did you notice many (or any) differences in your results with the different optimizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's adjust the learning rate.\n",
    "\n",
    "Let's stick with the `rmsprop` optimizer.\n",
    "\n",
    "We want to see how changing the learning rate (LR) affects things.\n",
    "RMSprop defaults with an LR of `0.001`, so we can try a decimal in both directions, `0.01` and `0.0001`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in order to tune the parameters of the optimizer we'll have to import it and instatiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3333 - accuracy: 0.9208\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1394 - accuracy: 0.9653\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1174 - accuracy: 0.9738\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.1048 - accuracy: 0.9782\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0896 - accuracy: 0.9823\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2286 - accuracy: 0.9741\n",
      "test_acc: 0.9740999937057495\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# set the LR when we instatiate the optimizerr\n",
    "rmsprop = optimizers.RMSprop(lr = 0.01) #start with 0.001 (default) to establish a baseline\n",
    "\n",
    "#pass our optimizer object below\n",
    "\n",
    "network.compile(optimizer=rmsprop,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What difference do you notice when you varied the Learning Rate?\n",
    "\n",
    "If you didn't notice much of a difference, try it again with more extreme amounts. Try `0.1` or `0.00001`\n",
    "\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.01 : 0.9740\n",
    "\n",
    "0.1 : 0.8858\n",
    "\n",
    "0.00001 : 0.8978\n",
    "\n",
    "0.0000001 : 0.0883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vary the Momentum.  \n",
    "\n",
    "To vary the momentum we'll have to use an optimizer that support momentum.  Let's head over to the keras documentation and choose one.\n",
    "\n",
    "I see that `SGD` or stochastic gradient descent uses momentum.  The value must be `>=0` so lets choose a few different values for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.9600 - accuracy: 0.7808\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.4539 - accuracy: 0.8858\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3742 - accuracy: 0.9000\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3361 - accuracy: 0.9091\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3116 - accuracy: 0.9148\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2888 - accuracy: 0.9213\n",
      "test_acc: 0.9212999939918518\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# set the LR when we instatiate the optimizer\n",
    "sgd = optimizers.SGD(momentum = 0.28) #always run the default first for a baseline value.\n",
    " \n",
    "#pass our optimizer object below\n",
    "\n",
    "network.compile(optimizer=sgd,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Values\n",
    "0.0 : 0.9121\n",
    "\n",
    "0.08 : 0.9161\n",
    "\n",
    "0.1 : 0.9158\n",
    "\n",
    "0.3 : 0.9204\n",
    "\n",
    "0.8 : 0.9480\n",
    "\n",
    "2.8 : 0.9212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These experiment can be extended by combining the epochs and batch size.\n",
    "\n",
    "Obviously they all interact with one another, so feel free to play with all of them now and try to get a feeling for what they do.\n",
    "\n",
    "If the learning rate is too low -- the model won't learn, but would increasing the epochs help with that?  What could be possible reasons to do this?\n",
    "\n",
    "How might you change your strategy based on how much data you have available to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

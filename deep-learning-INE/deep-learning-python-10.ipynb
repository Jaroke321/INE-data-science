{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse Of Dimensionality\n",
    "\n",
    "In this notebook we're going to explore how serious the curse of dimensionality is.  We'll do that by examining it from an experimental perspective.\n",
    "Let's take a look at the boston housing data found in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import boston_housing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train),(X_test, y_test) =  boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you can do to get an idea of your data is to examine the shape and print a few rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that our data has `403` samples and `13` features.  This is actually a pretty small dataset and the fact that it's got 13 features makes it a prime candidate to suffer from the curse of dimensionality.  13 features will mean that we have 13 variables to deal with, make it a 13 dimensional dataset.  This certainly creates a **lot** of sparsity.  \n",
    "\n",
    "Before we search for the effects of the curse, let's just take a look at the features themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Features\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks \n",
    "                 by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    \n",
    "    Target Value - Y labels\n",
    "    14. MEDV     Median value of owner-occupied homes in $1000's\n",
    "    \n",
    "    http://lib.stat.cmu.edu/datasets/boston\n",
    "    \n",
    "#### Note this dataset is OLD, from 1970's and obviously not racially sensitive at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02177</td>\n",
       "      <td>82.5</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415</td>\n",
       "      <td>7.610</td>\n",
       "      <td>15.7</td>\n",
       "      <td>6.2700</td>\n",
       "      <td>2.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>395.38</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.89822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.631</td>\n",
       "      <td>4.970</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3325</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>375.52</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.515</td>\n",
       "      <td>6.037</td>\n",
       "      <td>34.5</td>\n",
       "      <td>5.9853</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.69311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.376</td>\n",
       "      <td>88.4</td>\n",
       "      <td>2.5671</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>391.43</td>\n",
       "      <td>14.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0  1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "1  0.02177  82.5   2.03   0.0  0.415  7.610   15.7  6.2700   2.0  348.0   \n",
       "2  4.89822   0.0  18.10   0.0  0.631  4.970  100.0  1.3325  24.0  666.0   \n",
       "3  0.03961   0.0   5.19   0.0  0.515  6.037   34.5  5.9853   5.0  224.0   \n",
       "4  3.69311   0.0  18.10   0.0  0.713  6.376   88.4  2.5671  24.0  666.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     21.0  396.90  18.72  \n",
       "1     14.7  395.38   3.11  \n",
       "2     20.2  375.52   3.26  \n",
       "3     20.2  396.90   8.01  \n",
       "4     20.2  391.43  14.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I like to put things into pandas dataframe, for a lot of reasons.  Here I do it just because it will make the dataset print out nicely.\n",
    "cols = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",'DIS','RAD', 'TAX','PTRATIO','B','LSTAT']\n",
    "X_traindf = pd.DataFrame(X_train, columns = cols)\n",
    "X_traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the variables\n",
    "\n",
    "We need to standarize our data by scaling it and removing the mean and variance.  This is to address the problem of the data being on different scales, we'll discuss this in depth in the next lecture, for now, we'll just do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
       "        -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
       "         1.14850044,  0.44807713,  0.8252202 ],\n",
       "       [-0.40342651,  2.99178419, -1.33391162, -0.25683275, -1.21518188,\n",
       "         1.89434613, -1.91036058,  1.24758524, -0.85646254, -0.34843254,\n",
       "        -1.71818909,  0.43190599, -1.32920239],\n",
       "       [ 0.1249402 , -0.48361547,  1.0283258 , -0.25683275,  0.62864202,\n",
       "        -1.82968811,  1.11048828, -1.18743907,  1.67588577,  1.5652875 ,\n",
       "         0.78447637,  0.22061726, -1.30850006],\n",
       "       [-0.40149354, -0.48361547, -0.86940196, -0.25683275, -0.3615597 ,\n",
       "        -0.3245576 , -1.23667187,  1.10717989, -0.51114231, -1.094663  ,\n",
       "         0.78447637,  0.44807713, -0.65292624],\n",
       "       [-0.0056343 , -0.48361547,  1.0283258 , -0.25683275,  1.32861221,\n",
       "         0.15364225,  0.69480801, -0.57857203,  1.67588577,  1.5652875 ,\n",
       "         0.78447637,  0.3898823 ,  0.26349695]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Let's build a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # 1 output for regression, also no activation function!\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics =['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22e5628fac0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to set `verbose = 0` because we don't want 100 lines of epochs being complete.\n",
    "model.fit(X_train, y_train, batch_size=16, epochs= 100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 16.9642 - mse: 16.9642\n",
      "16.964195251464844\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print (test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do better?  \n",
    "\n",
    "At this point we've preprocessed the features a bit and now have a basic result.\n",
    "My question to you is -- can we do better?  The answer should be 'yes', but honestly I'm not sure how.\n",
    "One hypothesis I have is that since we have a very small dataset with 13 dimensions, I'm not convinced they are all helping more than they are hurting.  One simple approach would be to rank the features -- see which ones are better and then remove the worse ones.\n",
    "This will have the effect of keeping the best features while reducing the dimensionality of the data.\n",
    "\n",
    "Fortunately for us, Scikit-Learn comes equipped with a few functions to do exactly this kind of feature ranking.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest will choose the K best features, using a predefined univariate metric.  Don't worry too much about whats going on under the hood here, let's just accept that that there are some statistics we can use to assign value to the features.  SelectKBest will do that, and keep the `K` best that we ask for.  Go ahead and pick value for K below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_regression, k = 6)  # PICK A VALUE FOR K\n",
    "X_train_k = selector.fit_transform(X_train, y_train)  #we fit the selector on the training data only\n",
    "X_test_k = selector.transform(X_test)  # we use the fit selector to transform the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 6)\n",
      "(102, 6)\n"
     ]
    }
   ],
   "source": [
    "print (X_train_k.shape)\n",
    "print (X_test_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 14.8173 - mse: 14.8173\n",
      "loss is 14.817341804504395\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train_k.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # 1 output for regression, also no activation function!\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics =['mse'])\n",
    "\n",
    "model.fit(X_train_k, y_train, batch_size=16, epochs= 100, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test_k, y_test)\n",
    "print (\"loss is {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Did it get better?\n",
    "\n",
    "So you reduced the dimensionality of the data and now I want to ask you -- did it get better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should really be more systematic\n",
    "\n",
    "We should really be more systematic in checking how many dimensions are appropriate.\n",
    "The basic and most simple thing to do is just stick it all in a for loop.  I've done that for you below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's wrap our model creation and testing into a simple method.\n",
    "\n",
    "def run_model(X_train_k, y_train, X_test_k, y_test, i ):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(X_train_k.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))  # 1 output for regression, also no activation function!\n",
    "\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics =['mse'])\n",
    "\n",
    "    model.fit(X_train_k, y_train, batch_size=16, epochs= 100, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_k, y_test)\n",
    "    print (\"i is : {}, loss is {}\".format(i, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop will test all the values 1-13 of K.  This way we can get results for all the different combinations of features and see what actually does best.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 31.7469 - mse: 31.7469\n",
      "i is : 1, loss is 31.74686050415039\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 22.7886 - mse: 22.7886\n",
      "i is : 2, loss is 22.788585662841797\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.0661 - mse: 18.0661\n",
      "i is : 3, loss is 18.066062927246094\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.9735 - mse: 17.9735\n",
      "i is : 4, loss is 17.9735107421875\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.7555 - mse: 17.7555\n",
      "i is : 5, loss is 17.755508422851562\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 16.6649 - mse: 16.6649\n",
      "i is : 6, loss is 16.664934158325195\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 19.0514 - mse: 19.0514\n",
      "i is : 7, loss is 19.0513858795166\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 19.4761 - mse: 19.4761\n",
      "i is : 8, loss is 19.476139068603516\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 20.7485 - mse: 20.7485\n",
      "i is : 9, loss is 20.748472213745117\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 18.9705 - mse: 18.9705\n",
      "i is : 10, loss is 18.970489501953125\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 19.6428 - mse: 19.6428\n",
      "i is : 11, loss is 19.642810821533203\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 15.5450 - mse: 15.5450\n",
      "i is : 12, loss is 15.545031547546387\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 15.7879 - mse: 15.7879\n",
      "i is : 13, loss is 15.787901878356934\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,14):\n",
    "    selector = SelectKBest(f_regression, k = i)\n",
    "    X_train_k = selector.fit_transform(X_train, y_train)\n",
    "    X_test_k = selector.transform(X_test)\n",
    "    run_model(X_train_k, y_train, X_test_k, y_test, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So in conclusion, did reducing the dimensionality of the data help?\n",
    "\n",
    "Would you guess that it will always help? Always hurt? Why might it help and why might it not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The astute observer would notice...\n",
    "\n",
    "That these results are quite sporadically different.  Why do you think this might be?\n",
    "What happens if we run it all again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 30.7868 - mse: 30.7868\n",
      "i is : 1, loss is 30.786846160888672\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 24.6616 - mse: 24.6616\n",
      "i is : 2, loss is 24.661603927612305\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.3182 - mse: 17.3182\n",
      "i is : 3, loss is 17.31822967529297\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 16.8646 - mse: 16.8646\n",
      "i is : 4, loss is 16.864648818969727\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 17.9155 - mse: 17.9155\n",
      "i is : 5, loss is 17.915454864501953\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 15.7880 - mse: 15.7880\n",
      "i is : 6, loss is 15.788019180297852\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.1808 - mse: 18.1808\n",
      "i is : 7, loss is 18.180761337280273\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 23.0376 - mse: 23.0376\n",
      "i is : 8, loss is 23.037607192993164\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 18.7168 - mse: 18.7168\n",
      "i is : 9, loss is 18.716766357421875\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.0048 - mse: 21.0048\n",
      "i is : 10, loss is 21.004819869995117\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 8.1456 - mse: 8.1456WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 21.6238 - mse: 21.6238\n",
      "i is : 11, loss is 21.62381362915039\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.7402 - mse: 17.7402\n",
      "i is : 12, loss is 17.74018669128418\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 17.0244 - mse: 17.0244\n",
      "i is : 13, loss is 17.024444580078125\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,14):\n",
    "    selector = SelectKBest(f_regression, k = i)\n",
    "    X_train_k = selector.fit_transform(X_train, y_train)\n",
    "    X_test_k = selector.transform(X_test)\n",
    "    run_model(X_train_k, y_train, X_test_k, y_test, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Question\n",
    "Are the answers the same?  Why are these results less stable than the MNIST data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Project\n",
    "\n",
    "For the very determined amongst you I offer the following idea:\n",
    "\n",
    "Remember our MNIST dataset.  If you observe it carefully you will notice that a lot of the pixel values are 0, in fact they are just white space.  However every single pixel is another dimension added to our dataset.  Can we remove some of the pixels and still have the same data?\n",
    "Perhaps you do this by removing a row / column of each MNIST image.  Going from 28 x 28 (784,) --> 26 x 26 (676,) would drop 108 dimensions from the data.  In theory this should help our accuracy.\n",
    "\n",
    "Would it?\n",
    "You can setup the experiment and run it yourself.  You may notice that it does or does not depending on your network.  For example, I am doubtful that lowering the dimension would help it get past 97% test accuracy, but if you setup a dumber network (6 node input) it would probably help in that case.\n",
    "What I mean to say is, that it would most likely help the bottom line, but you may not see with a dense network that is already performing so well (97% accuracy).\n",
    "I won't do this, but I do encourage you to experiment and find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

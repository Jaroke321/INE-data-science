{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Loss Functions, Batch Size and Epochs\n",
    "\n",
    "In this notebook, we'll use the MNIST dataset to explore loss functions, the epoch, and batch parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MNIST data functions\n",
    "# matplotlib for plotting\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mnist.load_data() will automatically download the dataset if you don't have it\n",
    "(MNIST_train_X, MNIST_train_y), (MNIST_test_X, MNIST_test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've renamed our previous variables and introduced some more notation.  We always call the images `X`.  `X` represents two things, the letter 'x' is commonly used in machine learning to refer to the 'features' which is how we describe the data in numbers.  In the case of pictures, our feature are just the pixel values. The `X` is upper-case because it's , because `X` represents a matrix.  Each column of X is a single feature, and each row is a sample.\n",
    "\n",
    "In constrast, we always call our labels (answers) `y`.  It's just common notation to have `y` be the labels.  It's a lower-case `y` because it's a vector, a single list of labels.\n",
    "\n",
    "It goes without saying, but I'll say it anyways -- always make sure your `X` and `y` indices line up! If you are doing preprocessing and shuffling anything, you have to make sure to keep them lined up the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of preprocessing on the data before we can train (teach) our network\n",
    "\n",
    "For today, just ignore this.  Consider it a \"Necessary evil\". It's really not evil, but it is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_X = MNIST_train_X.reshape((60000, 28 * 28))\n",
    "MNIST_train_X = MNIST_train_X.astype('float32') / 255\n",
    "\n",
    "MNIST_test_X = MNIST_test_X.reshape((10000, 28 * 28))\n",
    "MNIST_test_X = MNIST_test_X.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MNIST_train_y = to_categorical(MNIST_train_y)\n",
    "MNIST_test_y = to_categorical(MNIST_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the architecture of the network\n",
    "\n",
    "Let's stick with the same one from the last excercise, this way we can see how it changes with different loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We compile the network with an optimizer, loss function and metric\n",
    "\n",
    "Now we'd like to compare some loss functions.\n",
    "Let's take a look at whats available in the box from Keras\n",
    "\n",
    "https://keras.io/losses/\n",
    "\n",
    "There are a bunch of losses here!\n",
    "\n",
    "It's actually a very nuanced decision to decide which loss to use, but the important point for you -- is that most of the ones designed for classification will work pretty well on MNIST.\n",
    "In fact even the ones not designed for classification will work well!\n",
    "\n",
    "Go ahead and try a few different ones.\n",
    "\n",
    "I suggest you try\n",
    "\n",
    "* `mape` (meant for regression)\n",
    "* `mse` (meant for regression)\n",
    "* `hinge` (meant for classification)\n",
    "* `categorical_hinge`  (meant for multi-class classification)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0120 - accuracy: 0.9223\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0053 - accuracy: 0.9663\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0037 - accuracy: 0.9766\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0028 - accuracy: 0.9828\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0023 - accuracy: 0.9862 0s - los\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0034 - accuracy: 0.9776\n",
      "test_acc: 0.9775999784469604\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "### Try different losses below\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='mse',             # change this parameter here.\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hinge: 0.97259998\n",
    "\n",
    "categorical_hinge: 0.97259998 \n",
    "\n",
    "MSE: 0.97759997\n",
    "\n",
    "MAPE: 0.966300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did you notice many (or any) differences in your results with the different loss functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here: Very little difference between the different loss functions. The mse loss function performed the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's adjust the batch size and epochs\n",
    "\n",
    "Let's stick with the loss function `categorical_crossentropy`\n",
    "This is the most commonly used loss function for this type of problem.\n",
    "\n",
    "We want to see how using different batch sizes and epochs affects the learning.\n",
    "\n",
    "Let's try a larger batch size first.\n",
    "Go ahead and run it with `1024` instead of `128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.2171 - accuracy: 0.9420\n",
      "Epoch 2/5\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.1308 - accuracy: 0.9711\n",
      "Epoch 3/5\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.1084 - accuracy: 0.9787\n",
      "Epoch 4/5\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.0944 - accuracy: 0.9819\n",
      "Epoch 5/5\n",
      "7500/7500 [==============================] - 60s 8ms/step - loss: 0.0834 - accuracy: 0.9851\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9760\n",
      "test_acc: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=8)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What difference do you notice when you varied the batch size.\n",
    "\n",
    "You should try a range of numbers, see if performance gets better or worse when you change the size of the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1024 : 0.9627 \n",
    "\n",
    "2048 : 0.9549\n",
    "\n",
    "4096 : 0.9298\n",
    "\n",
    "8192 : 0.9082"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64 : 0.9801\n",
    "\n",
    "32 : 0.9799\n",
    "\n",
    "16 : 0.9775\n",
    "\n",
    "8  : 0.9760"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next lets try the same experiment, but with the Epoch numbers\n",
    "\n",
    "We've been doing `5` epochs, let's try `10` and less.  We'll go back to the 128 batch size, for control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2570 - accuracy: 0.9256\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.1040 - accuracy: 0.9687\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0685 - accuracy: 0.9794\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0498 - accuracy: 0.9850\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0377 - accuracy: 0.9884 0s - loss: 0.0374 - accuracy: \n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0289 - accuracy: 0.9916\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0225 - accuracy: 0.9934\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0171 - accuracy: 0.9952 \n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0130 - accuracy: 0.9964 \n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0099 - accuracy: 0.9969\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0061 - accuracy: 0.9982\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 9.5778e-04 - accuracy: 0.9998- loss: 1.0000e-0\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1006 - accuracy: 0.9827\n",
      "test_acc: 0.982699990272522\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  # Dense is the same as fully connected.\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=20, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 : 0.9790\n",
    "\n",
    "20 : 0.9826\n",
    "\n",
    "1 :  0.9581"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You should be able to get a feel for whats going on.\n",
    "\n",
    "Try varying both epochs and batch sizes now.  You can even mix in a different loss function.\n",
    "Which parameter seems to affect performance the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here:\n",
    "\n",
    "The smaller your batch size, the fewer epochs you need, that's because we do more updates per epoch.\n",
    "\n",
    "The greater your batch size, the more epochs, because we do fewer updates per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, how much does the loss function, epochs and batch sizes matter?\n",
    "\n",
    "## Try to sum up your intuitions here.\n",
    "\n",
    "Your answer here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

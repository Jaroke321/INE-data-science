{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding One Hot Encoding\n",
    "\n",
    "In this notebook we are going to explore why we do one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Let's setup our standard approach to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mnist.load_data() will automatically download the dataset if you don't have it\n",
    "(MNIST_train_X, MNIST_train_y), (MNIST_test_X, MNIST_test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_X = MNIST_train_X.reshape((60000, 28 * 28))\n",
    "MNIST_test_X = MNIST_test_X.reshape((10000, 28 * 28))\n",
    "\n",
    "MNIST_train_X = MNIST_train_X.astype('float32') / 255\n",
    "MNIST_test_X = MNIST_test_X.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "So here is the OHE that we have been doing until now.  Let's take a look at the before and after to convince ourselves of what it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(MNIST_train_y[:5])\n",
    "print(MNIST_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "MNIST_train_y_ohe = to_categorical(MNIST_train_y)\n",
    "MNIST_test_y_ohe = to_categorical(MNIST_test_y)\n",
    "\n",
    "print(MNIST_train_y_ohe[:5])\n",
    "print(MNIST_train_y_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the difference in shape mean?\n",
    "\n",
    "We see that the non-encoded labels are shape (60000,) which means they are a 1-tensor, or a column vector.\n",
    "This means that each label is a single integer.  This makes sense, it's just the list of answers.\n",
    "\n",
    "Now once we encode, then the labels become a 2-tensor, or a matrix.  Each label is now a one-hot-encoded vector, this makes the dimensionality of the labels MUCH higher (10x higher), but allows for the network to learn each label seperately from each other.  It assumes no relationship between them.\n",
    "\n",
    "We have seen the results from OHE many times, we get around `%97` accuracy.  So what about just leaving the labels as they are?  \n",
    "\n",
    "Would we get better results by not increasing the dimesionality of our labels?\n",
    "\n",
    "### Changes we have to make in order to use scalar labels\n",
    "\n",
    "Ok, so if we are going to make the output a scalar value (not a vector), then our final label is shape (1,) because it's a single label.\n",
    "This means we need to change out final layer to be `Dense(1, activation = None)`.  We choose `1` for the final output because that's the shape.  We also have to remove the `softmax` because that is designed to make a probablisitic output across a vector, but our output will be a single value.  So we can then basically do regression on the labels.  \n",
    "\n",
    "Let's let our model just try to learn the answer as if they are ordinal (greater and less than each other).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the non OHE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.1250 - accuracy: 0.1540 0s - loss: 1.1394 - accu\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.8117 - accuracy: 0.1679\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.7080 - accuracy: 0.1726\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.6521 - accuracy: 0.1756\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.6031 - accuracy: 0.1781 1s - loss:\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7047 - accuracy: 0.1855\n",
      "test_acc: 0.18549999594688416\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  \n",
    "network.add(layers.Dense(1, activation=None))  # two important changes here.\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='mae',  # have to use a regression loss function herer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "So, what did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Experiment\n",
    "\n",
    "So maybe the problem was that we used the default labels 0-9.  What if we re-ordered the numbers so they made more sense? Like if we sorted the digits by similiarity and remapped them so the regression could find the patterns better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_test_y = pd.Series(MNIST_test_y)\n",
    "MNIST_train_y = pd.Series(MNIST_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make A Mapping\n",
    "\n",
    "Fill out the dictionary below to make a mapping.  The number on the left side is the original value, the number on the right side is the \"new\" value.  Try to order it so it makes a case for digits being physically similar in shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { 9:0,\n",
    "            4:1,\n",
    "            7:2,\n",
    "            8:3,\n",
    "            1:4,\n",
    "            6:5,\n",
    "            5:6, \n",
    "            2:7,\n",
    "            3:8,\n",
    "            0:9 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data: \n",
      "0    5\n",
      "1    0\n",
      "2    4\n",
      "3    1\n",
      "4    9\n",
      "5    2\n",
      "6    1\n",
      "7    3\n",
      "8    1\n",
      "9    4\n",
      "dtype: uint8\n",
      " ----- \n",
      "the reordered data: \n",
      "0    6\n",
      "1    9\n",
      "2    1\n",
      "3    4\n",
      "4    0\n",
      "5    7\n",
      "6    4\n",
      "7    8\n",
      "8    4\n",
      "9    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "MNIST_train_y_reorder = MNIST_train_y.map(mapping)\n",
    "MNIST_test_y_reorder = MNIST_test_y.map(mapping)\n",
    "print (\"The original data: \\n{}\".format(MNIST_train_y[:10]))\n",
    "print (\" ----- \")\n",
    "print (\"the reordered data: \\n{}\".format(MNIST_train_y_reorder[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 1.9722 - accuracy: 0.1226\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.9566 - accuracy: 0.1427\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.7412 - accuracy: 0.1482\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.6175 - accuracy: 0.1524\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.5400 - accuracy: 0.1567\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5804 - accuracy: 0.1595\n",
      "test_acc: 0.15950000286102295\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))  \n",
    "network.add(layers.Dense(1, activation=None))  # two important changes here.\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='mse',  # have to use a regression loss function here\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y_reorder, epochs=5, batch_size=128)\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y_reorder)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Conclusion:\n",
    "\n",
    "Did it help? Hurt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final experiments\n",
    "\n",
    "One other idea for you : maybe you need to add more layers.  Try a more complex network and see if that helps the regression case work better.  I'd consider something more like `256 >> 128 >> 64 >> 10 > 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 1.8695 - accuracy: 0.1401\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.7964 - accuracy: 0.1730\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.5653 - accuracy: 0.1790\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.4331 - accuracy: 0.1828\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.3456 - accuracy: 0.1847\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3633 - accuracy: 0.1838\n",
      "test_acc: 0.18379999697208405\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(784,)))  \n",
    "network.add(layers.Dense(128, activation='relu'))  \n",
    "network.add(layers.Dense(64, activation='relu'))  \n",
    "network.add(layers.Dense(16, activation='relu'))  \n",
    "\n",
    "network.add(layers.Dense(1, activation=None))  # two important changes here.\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='mse',  # have to use a regression loss function herer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y_reorder, epochs=5, batch_size=128)\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y_reorder)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same experiment - no re-mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 2.4062 - accuracy: 0.1827\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.9951 - accuracy: 0.2005\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.6947 - accuracy: 0.2038\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.5085 - accuracy: 0.2051\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.3992 - accuracy: 0.2065\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6146 - accuracy: 0.2088\n",
      "test_acc: 0.20880000293254852\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential() #we'll stick to sequential for this course\n",
    "\n",
    "network.add(layers.Dense(256, activation='relu', input_shape=(784,)))  \n",
    "network.add(layers.Dense(128, activation='relu'))  \n",
    "network.add(layers.Dense(64, activation='relu'))  \n",
    "network.add(layers.Dense(16, activation='relu'))  \n",
    "\n",
    "network.add(layers.Dense(1, activation=None))  # two important changes here.\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='mse',  # have to use a regression loss function herer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "network.fit(MNIST_train_X, MNIST_train_y, epochs=5, batch_size=128)\n",
    "test_loss, test_acc = network.evaluate(MNIST_test_X, MNIST_test_y)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
